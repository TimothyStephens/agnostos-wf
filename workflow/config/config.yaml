# This file should contain everything to configure the workflow on a global scale.

## Major absolute path prefixes
# Workflow absolute path
wdir: "/home/timothy/GitHub/agnostos-wf/workflow"
# Results absolute path
rdir: "/home/timothy/GitHub/agnostos-wf/results"
# Community cluster absolute path (can be that same as rdir)
idir: "/home/timothy/GitHub/agnostos-wf/results"
# Database absolute path
ddir: "/home/timothy/GitHub/agnostos-wf/databases"

# Absolute path to existing cluster DB (sequence and cluster DBs) - only used for the "update" module
ordir: "/home/timothy/GitHub/agnostos-wf/results/clusterDB_results"

## Local template folder
local_tmp: &tmpdir "/home/timothy/GitHub/agnostos-wf/tmp"

## Setting module to "update" to run the cluster-update version of the snakemake workflow (otherwise "creation")
module: "creation" #"update"

## Choose a name for your dataset
data_name: "tara_039_041"

## Absolute path to the sequences that will be clustered or integrated into an existing cluster DB
# Formats: name_contigs.fasta or name_proteins.fasta
#   OR     name_contigs.fasta.gz or name_proteins.fasta.gz
sequences: "/home/timothy/GitHub/agnostos-wf/agnostos_test/db_creation_data/TARA_039_041_SRF_0.1-0.22_5K_contigs.fasta"
## Specify which type of sequence you have, can be either "proteins" or "contigs"
sequence_type: "contigs" #"contigs" or "proteins" or "anvio_genes"
## If you alrady have the protein predictions, please provide path to protein completeness information (i.e., complete or partial ORFs)
# In case your data comes from an Anvi'o contigDB, please specify here the anvio gene_calls.tsv file,
# retrieved via "anvi-export-gene-calls -c CONTIGS.db -o anvio_gene_calls.tsv"
# If you are providing just contigs, then leave this empty.
sequences_partial: ""

## If you want to classify the singleton in the four category set the following entry to "true"
singl: "true"

## If you want to re-validate and classify the existing GCs that got integrated with new genes
eval_shared: "true"
# all shared GCs is the default, other possibilities are:
#"discarded" for reprocessing all the previously discarded GCs with new sequences
#"increase" for reprocessing all the shared GCs with 30% new sequences
shared_type: "all" # "discarded" # "increase"

# If you want ot dowload the required DBs at each step and then remove them, set db_mode to memory.
db_mode: "memory" # Options: "memory", "all"

conda_env: "../envs/workflow.yml"

# Threads configuration
threads_default: 28
threads_collect: 28
threads_cat_ref: 28
threads_res: 14

# Databases (paths are relative from "ddir")
pfam_db: "Pfam-A.hmm"
pfam_clan: "Pfam-A.clans.tsv.gz"
antifam_db: "AntiFam.hmm"
uniref90_db: "uniref90.db"
nr_db: "nr.db"
uniclust_db: "UniRef30_2023_02"
pfam_hh_db: "pfam"
DPD: "dpd_uniprot_sprot.fasta.gz"
# Taxonomy for the gene cluster sequences
conf_tax_db: "gtdb"
gtdb_tax: "gtdb-r89_54k/gtdb-r89_54k.fmi"
nr_tax: "nr_euk_db/kaiju_db_nr_euk.fmi"
# Files retrieved from the databases
# List of shared reduced Pfam domain names (dowloadable from Figshare..)
pfam_shared_terms: "Pfam-34_names_mod_20102021.tsv"
# Created using the protein accessions and the descriptions found on the fasta headers
uniref90_prot: "uniref90.proteins.tsv.gz"
nr_prot: "nr.proteins.tsv.gz"
# Information dowloaded from Dataset-S1 from the DPD paper:
dpd_info: "dpd_ids_all_info.tsv.gz"
# Mutant phenotype Databases
## amino acid sequences
aa_mutants: "aaseqs"
## contextual data for the fitness experiments
mutantDB: "feba.db"

# MPI runner (de.NBI cloud, SLURM)
mpi_runner: "srun --mpi=pmi2"

#vmtouch for the DBs
vmtouch: "vmtouch"

# Gene prediction
prodigal_bin: "prodigal"
prodigal_mode: "meta" # for metagenomic data or "normal" for genomic data

# Annotation
hmmer_bin: "hmmsearch"

# Clustering config
mmseqs_bin: "mmseqs"
mmseqs_tmp: *tmpdir
mmseqs_local_tmp: *tmpdir
mmseqs_split_mem: "100G"
mmseqs_split: 10

# Clustering results config
seqtk_bin: "seqtk"

# Spurious and shadows config
hmmpress_bin: "hmmpress"

# Compositional validation config
datamash_bin: "datamash"
famsa_bin: "famsa"
odseq_bin: "OD-seq"
parasail_bin: "parasail_aligner"
parallel_bin: "parallel"
igraph_lib: "export LD_LIBRARY_PATH=/home/timothy/GitHub/agnostos-wf/bin/igraph/lib:${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}"
parasail_lib: "export LD_LIBRARY_PATH=/home/timothy/GitHub/agnostos-wf/lib:${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}"

# Cluster classification config
seqkit_bin: "seqkit"
filterbyname: "filterbyname.sh"
hhcons_bin: "hhconsensus"

# Cluster category refinement
hhsuite: "hh-suite"
hhblits_bin_mpi: "hhblits_mpi"
hhmake: "hhmake"
hhblits_prob: 90
hypo_filt: 1.0

# Taxonomy
kaiju_bin: "kaiju"

# Eggnog
eggnog_bin: "emapper.py"

# Cluster communities
hhblits_bin: "hhblits"
hhsearch_bin: "hhsearch"

